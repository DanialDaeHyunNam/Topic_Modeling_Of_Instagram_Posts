{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project : Instagram Data Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Introduce\n",
    "### 1.1 Purpose : \n",
    "\n",
    "### 1.2 Data set:\n",
    "   > - Train Data : 43264 rows, 20 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import private_function as pf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.under_sampling import *\n",
    "from imblearn.over_sampling import *\n",
    "from imblearn.combine import *\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_x_train_and_features_name(df):\n",
    "    cnt_vectorizer = CountVectorizer(lowercase=False)\n",
    "    X_train = cnt_vectorizer.fit_transform(df)\n",
    "#     cnt_feature_names = cnt_vectorizer.get_feature_names()\n",
    "    return X_train\n",
    "\n",
    "def run_lda(df, n_topic, max_iter = 100):\n",
    "    cnt_vectorizer = CountVectorizer(lowercase=False)\n",
    "    X_train = cnt_vectorizer.fit_transform(df)\n",
    "    cnt_feature_names = cnt_vectorizer.get_feature_names()\n",
    "\n",
    "    # hyper parameter\n",
    "    alpha = 1\n",
    "    beta = 1\n",
    "\n",
    "    # train the model\n",
    "    lda = LatentDirichletAllocation(n_components=n_topic, doc_topic_prior=alpha,\\\n",
    "                                    topic_word_prior=beta, learning_method='online', max_iter=max_iter)\n",
    "\n",
    "    %time lda.fit_transform(X_train)\n",
    "    \n",
    "    return lda, cnt_vectorizer, cnt_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "['직장인', '럽스타그램', '서울맛집', '먹스타그램', '친스타그램', '여행', '일상', '셀스타그램']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "['seoul']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "job.csv : 2213"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_new_5.csv : 2136"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_new_4.csv : 2148"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_new_3.csv : 2172"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_new_2.csv : 465"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_new_1.csv : 2029"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_new.csv : 2203"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_train.csv : 5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_food_1.csv : 2217"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_food.csv : 1911"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "food.csv : 1986"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "beer.csv : 1970"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "food_1.csv : 2068"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "food_2.csv : 2089"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "food_3.csv : 2046"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "friend.csv : 1888"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "trip_1.csv : 1145"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "trip.csv : 2533"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "daily.csv : 2195"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "daily_2.csv : 785"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "daily_1.csv : 2312"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "selfie.csv : 2156"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "seongnam.csv : 960"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "seoul_2.csv : 19"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "seoul_1.csv : 960"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "incheon.csv : 960"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "seoul.csv : 880"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "yongin.csv : 960"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "gyeonggido.csv : 960"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "Df list length : 29\n",
      "Df length changes after concat (if 0 means all datas are unique) : 0\n",
      "Df length changes after concat (if 0 means all datas are unique) : 0\n",
      "Df length changes after concat (if 0 means all datas are unique) : 1095\n",
      "Df length changes after concat (if 0 means all datas are unique) : 411\n",
      "Df length changes after concat (if 0 means all datas are unique) : 54\n",
      "Df length changes after concat (if 0 means all datas are unique) : 131\n",
      "Df length changes after concat (if 0 means all datas are unique) : 0\n",
      "Df length changes after concat (if 0 means all datas are unique) : 35\n",
      "Df length changes after concat (if 0 means all datas are unique) : 16\n",
      "Df length changes after concat (if 0 means all datas are unique) : 75\n",
      "Df length changes after concat (if 0 means all datas are unique) : 11\n",
      "Df length changes after concat (if 0 means all datas are unique) : 42\n",
      "Df length changes after concat (if 0 means all datas are unique) : 358\n",
      "Df length changes after concat (if 0 means all datas are unique) : 19\n",
      "Df length changes after concat (if 0 means all datas are unique) : 176\n",
      "Df length changes after concat (if 0 means all datas are unique) : 15\n",
      "Df length changes after concat (if 0 means all datas are unique) : 0\n",
      "Df length changes after concat (if 0 means all datas are unique) : 55\n",
      "Df length changes after concat (if 0 means all datas are unique) : 20\n",
      "Df length changes after concat (if 0 means all datas are unique) : 304\n",
      "Df length changes after concat (if 0 means all datas are unique) : 286\n",
      "Df length changes after concat (if 0 means all datas are unique) : 0\n",
      "Df length changes after concat (if 0 means all datas are unique) : 0\n",
      "Df length changes after concat (if 0 means all datas are unique) : 1\n",
      "Df length changes after concat (if 0 means all datas are unique) : 1\n",
      "Df length changes after concat (if 0 means all datas are unique) : 1\n",
      "Df length changes after concat (if 0 means all datas are unique) : 0\n",
      "Df length changes after concat (if 0 means all datas are unique) : 1\n",
      "df_train shape : (43264, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(46899, 14241)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_li = pf.getAllDataFrame()\n",
    "train = pf.makeOneTrainDf(df_li)\n",
    "train = pf.make_df_i_want(train)\n",
    "counter = pf.get_counter(train)\n",
    "counter.most_common()\n",
    "tag_count = 0\n",
    "for tup in counter.most_common():\n",
    "    tag_count += tup[1]\n",
    "tag_count, len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = train[train.tags_cnt == 0]\n",
    "# df_test = test.caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"caption_only\"] = train[\"caption_only\"].apply(lambda a: \"\" if type(a) == float else a)\n",
    "# train[\"caption_only\"] = train[\"caption_only\"].apply(lambda a: a.strip())\n",
    "# train = train[train[\"caption_only\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38791, 4473)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = train[train[\"tags_str\"] == \"\"][\"tags_str\"].values\n",
    "X_train = train[train[\"tags_str\"] != \"\"][\"tags_str\"].values\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lda_model(n_topic):\n",
    "    n_topic = str(n_topic)\n",
    "    lda = joblib.load(\"./lda_model\" + n_topic + \".pkl\")\n",
    "    cnt_feature_name = joblib.load(\"./lda_model\" + n_topic + \"_feature_name.pkl\")\n",
    "    return lda, cnt_feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 391 ms, sys: 29.4 ms, total: 420 ms\n",
      "Wall time: 431 ms\n",
      "CPU times: user 381 ms, sys: 25.8 ms, total: 407 ms\n",
      "Wall time: 418 ms\n",
      "CPU times: user 428 ms, sys: 36.2 ms, total: 464 ms\n",
      "Wall time: 482 ms\n",
      "CPU times: user 432 ms, sys: 32 ms, total: 464 ms\n",
      "Wall time: 480 ms\n",
      "CPU times: user 378 ms, sys: 30 ms, total: 408 ms\n",
      "Wall time: 418 ms\n"
     ]
    }
   ],
   "source": [
    "%time lda_model11, cnt_feature_names11 = load_lda_model(11)\n",
    "%time lda_model12, cnt_feature_names12 = load_lda_model(12)\n",
    "%time lda_model13, cnt_feature_names13 = load_lda_model(13)\n",
    "%time lda_model14, cnt_feature_names14 = load_lda_model(14)\n",
    "%time lda_model15, cnt_feature_names15 = load_lda_model(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "여행에미치다 여행스타그램 여행 travel korea photography photo 일본 seoul 냥스타그램 감성사진 감성 trip 한국 취미 풍경 고양이 스냅 사랑 여행그램\n",
      "Topic 1:\n",
      "일상 맞팔 데일리 소통 선팔 셀스타그램 좋아요 셀카 셀피 팔로우 daily 먹스타그램 얼스타그램 일상스타그램 선팔하면맞팔 오오티디 인친 좋아요반사 럽스타그램 ootd\n",
      "Topic 2:\n",
      "제주도 제주도맛집 제주맛집 선팔맞팔 제주 소통해요 서귀포맛집 f4follow 제주여행 소통하자 제주맛집추천 제주도그램 제주서귀포맛집 좋아 제주도여행 평일 맥주스타그램 휴일 인스타감성 제주도흑돼지맛집\n",
      "Topic 3:\n",
      "맥주스타그램 술스타그램 맥주 토토사이트추천 술안주 럽스타 토토사이트 스포츠가족방 다리다리가족방 파워볼가족방 사다리가족방 프로토가족방 사설사이트추천 픽스터가족방 사설사이트 운동하는남자 일상스냅 beer selfcam 육아템\n",
      "Topic 4:\n",
      "남친이랑 데이트 럽스타그램 럽스타 휴가 커플 사랑해 커플스타그램 행복 남자친구 여름휴가 영화 연애중 연애 남친 고마워 사랑꾼 힐링 ㅋㅋㅋ 화이팅\n",
      "Topic 5:\n",
      "bts kpop 방탄소년단 jungkook jimin exo jhope korea jin 귀여운 suga army bangtanboys taehyung blackpink 여자 kimtaehyung BTS jeonjungkook rm\n",
      "Topic 6:\n",
      "서울맛집 강남맛집 강남 강남역 신용카드현금화 곱창 카드깡 강남역맛집 역삼맛집 역삼동맛집 신논현맛집 대치동맛집 곱창맛집 상품권현금화 역삼동 서초맛집 양재맛집 서초동맛집 역삼역맛집 역삼역\n",
      "Topic 7:\n",
      "다이어트 운동하는여자 운동 헬스 다이어터 다이어트식단 운동하는남자 용인 diet 성남 이벤트 헬스타그램 다이어트그램 식단 Repost 필라테스 건강 fitness 몸스타그램 운스타그램\n",
      "Topic 8:\n",
      "반려견 멍스타그램 강아지 개스타그램 dog 댕댕이 펫스타그램 견스타그램 dogstagram 네일아트 puppy 독스타그램 pet 꽃스타그램 산책 포메 꽃다발 말티즈 푸들 젤네일\n",
      "Topic 9:\n",
      "먹스타그램 먹방 맛스타그램 맛집 서울맛집 food 점심 먹스타 instafood foodstagram 디저트 음식 먹방스타그램 먹부림 푸드스타그램 부산맛집 맛스타 맛있다 JMT 존맛\n",
      "Topic 10:\n",
      "럽스타그램 육아스타그램 육아 육아소통 육아맘 맘스타그램 도치맘 젊줌마 딸스타그램 인스타베이비 애스타그램 일상 아들스타그램 아들맘 사랑해 직장인 겐조 커플 세젤귀 워킹맘\n"
     ]
    }
   ],
   "source": [
    "pf.display_topics(lda_model11, cnt_feature_names11, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "일상 맞팔 소통 데일리 선팔 셀스타그램 좋아요 셀카 셀피 팔로우 먹스타그램 daily 얼스타그램 일상스타그램 선팔하면맞팔 오오티디 인친 럽스타그램 좋아요반사 ootd\n",
      "Topic 1:\n",
      "여행에미치다 여행스타그램 여행 travel 휴가 바다 photography 여름휴가 감성사진 일본 trip 풍경 웨딩 럽스타그램 힐링 photo 스냅 예신 웨딩촬영 결혼\n",
      "Topic 2:\n",
      "럽스타그램 육아스타그램 육아 육아소통 육아맘 맘스타그램 사랑해 도치맘 젊줌마 딸스타그램 직장인 인스타베이비 애스타그램 아들스타그램 일상 아들맘 세젤귀 워킹맘 줌마그램 딸맘\n",
      "Topic 3:\n",
      "제주도 제주도맛집 제주맛집 f4follow 선팔맞팔 소통해요 제주 소통하자 서귀포맛집 제주여행 맥주스타그램 제주맛집추천 제주도그램 제주서귀포맛집 좋아 제주도여행 평일 휴일 인스타감성 제주도흑돼지맛집\n",
      "Topic 4:\n",
      "서울맛집 겐조 커플 샤넬 핫플레이스 구찌 에르메스 팔찌 발렌티노 대구맛집 프라다 부산맛집 까르띠에 펜디 골든구스 몽블랑 불가리 막스마라 서울여행 보태가베네타\n",
      "Topic 5:\n",
      "신용카드현금화 카드깡 상품권현금화 소액결제 소액결제현금화 모바일문화상품권 타투 굿핀 해피머니 컬쳐랜드 일수대출 신용카드대출 홍대맛집 모바 휴대폰소액결제 남친이랑 차스타그램 성남 핸드폰소액결제 빈티지\n",
      "Topic 6:\n",
      "강남맛집 곱창 강남역 서울맛집 강남 강남역맛집 역삼맛집 역삼동맛집 신논현맛집 대치동맛집 곱창맛집 역삼동 서초맛집 양재맛집 서초동맛집 역삼역맛집 역삼역 강남역맛집베스트10 양재역맛집 서초동곱창\n",
      "Topic 7:\n",
      "korea seoul 한국 kpop 韓国 사랑 art 그림 글스타그램 일러스트 bts 공감 글귀 beautiful korean いいね返し 생각 독서 exo 감성\n",
      "Topic 8:\n",
      "반려견 멍스타그램 강아지 개스타그램 냥스타그램 아이폰케이스 dog 댕댕이 고양이 펫스타그램 폰케이스 견스타그램 아이폰8 커플케이스 cat dogstagram 아이폰7케이스 아이폰x케이스 puppy 독스타그램\n",
      "Topic 9:\n",
      "토토사이트추천 다이어트 운동하는남자 럽스타 운동하는여자 술스타그램 토토사이트 스포츠가족방 사다리가족방 파워볼가족방 사설사이트추천 사설사이트 픽스터가족방 프로토가족방 다리다리가족방 술안주 일상스냅 selfcam 육아템 당첨\n",
      "Topic 10:\n",
      "먹스타그램 먹방 맛스타그램 맛집 남친이랑 맥주스타그램 food 점심 술스타그램 먹스타 맥주 instafood foodstagram 음식 데이트 먹방스타그램 존맛 먹부림 푸드스타그램 디저트\n",
      "Topic 11:\n",
      "선물 마카롱 취미 용인 기념일 생일 핸드메이드 귀걸이 생일선물 꽃스타그램 꽃다발 디저트 악세사리 원데이클래스 케이크 flower 커플링 여친선물 cake 마카롱맛집\n"
     ]
    }
   ],
   "source": [
    "pf.display_topics(lda_model12, cnt_feature_names12, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "먹스타그램 먹방 맛스타그램 일상 럽스타그램 맛집 직장인 육아스타그램 육아 육아소통 육아맘 food 맘스타그램 점심 도치맘 사랑해 젊줌마 딸스타그램 먹스타 인스타베이비\n",
      "Topic 1:\n",
      "축구 커플신발 푸마 해외직구 레플 아웃도어 헬스 커플운동화 나이키에어맥스 신발쇼핑몰 운동화할인 아식스 연예인골든구스 등산화 신발도매 신상신발 명품등산화 유행신발 GGDB 운동화도매\n",
      "Topic 2:\n",
      "속초 워터파크 속초맛집 그래도 속초여행 거제도맛집 거제맛집 꾸욱 속초중앙시장맛집 2018 당일치기여행 인친대환영 안산맛집 삼척맛집 캐리비안베이 클래식 속초대게맛집 원피스 속초중앙시장 대부도맛집\n",
      "Topic 3:\n",
      "제주도 제주도맛집 제주맛집 제주 서귀포맛집 제주여행 f4follow 선팔맞팔 제주맛집추천 소통해요 소통하자 제주도그램 제주서귀포맛집 맥주스타그램 제주도여행 제주도흑돼지맛집 제주흑돼지 제주도흑돼지 서귀포흑돼지맛집 서귀포흑돼지\n",
      "Topic 4:\n",
      "일상 맞팔 소통 데일리 선팔 셀스타그램 좋아요 셀카 셀피 팔로우 먹스타그램 얼스타그램 일상스타그램 daily 선팔하면맞팔 오오티디 인친 좋아요반사 럽스타그램 ootd\n",
      "Topic 5:\n",
      "강남맛집 곱창 강남역 강남 서울맛집 맛스타그램 강남역맛집 역삼맛집 역삼동맛집 신논현맛집 대치동맛집 곱창맛집 역삼동 서초맛집 양재맛집 서초동맛집 역삼역맛집 역삼역 강남역맛집베스트10 양재역맛집\n",
      "Topic 6:\n",
      "맥주스타그램 술스타그램 맥주 beer 혼술 치맥 수제맥주 소주 소맥 치킨 맥주한잔 술집 beerstagram 크래프트비어 맥주그램 존맛탱 낮술 생맥주 술스타 craftbeer\n",
      "Topic 7:\n",
      "뉴스킨 피부관리 갈바닉 루미스파 스킨케어 포천여행 전주한옥마을 홈케어 짖어야개다 포천카페 효자동맛집 햄스터 피부고민 포천이동갈비 성남애견미용 뉴스킨하세요 평일 180도 물광피부 2018년\n",
      "Topic 8:\n",
      "여행에미치다 여행 여행스타그램 travel 휴가 korea photography 럽스타그램 일본 photo seoul 바다 여름휴가 한국 trip 감성사진 풍경 웨딩 스냅 예신\n",
      "Topic 9:\n",
      "남친이랑 서울맛집 데이트 부산맛집 핫플레이스 맛집투어 JMT 서울여행 대구맛집 럽스타그램 대전맛집 홍대맛집 선물 용산맛집 숙대맛집 남영동맛집 seoullife 카드깡 맛집골목 후암동맛집\n",
      "Topic 10:\n",
      "아이폰케이스 용인 폰케이스 아이폰8 성남 커플케이스 아이폰7케이스 아이폰x케이스 아이폰x 아이폰8케이스 이벤트 핸드폰케이스 갤럭시케이스 아이폰6케이스 아이폰7플러스케이스 아이폰8플러스케이스 아이폰7 판교 분당 케이스\n",
      "Topic 11:\n",
      "bts kpop 방탄소년단 jungkook jimin exo jhope jin 귀여운 suga army bangtanboys taehyung korea blackpink 여자 kimtaehyung BTS jeonjungkook rm\n",
      "Topic 12:\n",
      "토토사이트추천 겐조 운동하는남자 럽스타 샤넬 커플 구찌 팔찌 에르메스 토토사이트 발렌티노 술스타그램 스포츠가족방 사설사이트 사설사이트추천 파워볼가족방 프로토가족방 사다리가족방 픽스터가족방 다리다리가족방\n"
     ]
    }
   ],
   "source": [
    "pf.display_topics(lda_model13, cnt_feature_names13, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "여행에미치다 여행스타그램 여행 travel 휴가 korea photography 일본 photo seoul 바다 감성사진 한국 trip 풍경 웨딩 럽스타그램 스냅 예신 결혼\n",
      "Topic 1:\n",
      "신용카드현금화 카드깡 상품권현금화 소액결제 소액결제현금화 모바일문화상품권 굿핀 해피머니 컬쳐랜드 신용카드대출 일수대출 모바 홍대맛집 휴대폰소액결제 차스타그램 핸드폰소액결제 빈티지 골프 상품권 비트코인\n",
      "Topic 2:\n",
      "다이어트 운동하는여자 다이어터 다이어트식단 용인 diet 다이어트그램 꽃다발 꽃스타그램 식단 헬스 flower 운동 운스타그램 fitness 플로리스트 유지어터 workout 운동하는남자 식단일기\n",
      "Topic 3:\n",
      "축구 헬스 커플신발 푸마 레플 해외직구 아웃도어 커플운동화 나이키에어맥스 신발쇼핑몰 신상신발 등산화 신발도매 명품등산화 유행신발 아디다스신발 GGDB 아식스 아디다스울트라부스트 NEWBALANCE\n",
      "Topic 4:\n",
      "kpop bts exo 귀여운 blackpink 여자 korea kawaii 아름다운 自撮り boy asmr korean 속초맛집 Asia jisoo kpopl4l tomboy селфи 自分撮り\n",
      "Topic 5:\n",
      "서울맛집 부산맛집 강남맛집 대구맛집 곱창 홍대맛집 강남역 강남역맛집 대전맛집 강남 맛스타그램 역삼맛집 이태원맛집 역삼동맛집 신논현맛집 대치동맛집 서면맛집 곱창맛집 역삼동 서초맛집\n",
      "Topic 6:\n",
      "남친이랑 데이트 커플 사랑해 럽스타 커플스타그램 남자친구 영화 연애중 연애 행복 남친 고마워 사랑꾼 ㅋㅋㅋ 행복해 데이트그램 사랑 여자친구 화이팅\n",
      "Topic 7:\n",
      "럽스타그램 육아 육아스타그램 육아소통 육아맘 맘스타그램 도치맘 일상 젊줌마 딸스타그램 인스타베이비 애스타그램 직장인 아들스타그램 사랑해 아들맘 세젤귀 반려견 워킹맘 멍스타그램\n",
      "Topic 8:\n",
      "토토사이트추천 겐조 샤넬 서울맛집 운동하는남자 럽스타 핫플레이스 커플 구찌 에르메스 술스타그램 토토사이트 발렌티노 팔찌 스포츠가족방 사설사이트추천 파워볼가족방 프로토가족방 픽스터가족방 사설사이트\n",
      "Topic 9:\n",
      "선물 직장인 아이폰케이스 냥스타그램 마카롱 취미 고양이 폰케이스 아이폰8 커플케이스 공감 그림 글스타그램 기념일 디저트 일러스트 아이폰7케이스 아이폰x케이스 글귀 귀걸이\n",
      "Topic 10:\n",
      "먹스타그램 먹방 맛스타그램 맛집 맥주스타그램 food 술스타그램 점심 먹스타 맥주 instafood foodstagram 먹방스타그램 음식 존맛 맛스타 먹부림 푸드스타그램 맛있다 디저트\n",
      "Topic 11:\n",
      "일상 맞팔 데일리 소통 선팔 셀스타그램 좋아요 셀카 셀피 팔로우 먹스타그램 daily 얼스타그램 일상스타그램 선팔하면맞팔 오오티디 럽스타그램 인친 좋아요반사 ootd\n",
      "Topic 12:\n",
      "제주도 제주도맛집 제주맛집 제주 서귀포맛집 제주여행 f4follow 선팔맞팔 제주맛집추천 소통해요 소통하자 제주도그램 제주서귀포맛집 맥주스타그램 제주도여행 제주도흑돼지맛집 제주흑돼지 제주도흑돼지 서귀포흑돼지맛집 서귀포흑돼지\n",
      "Topic 13:\n",
      "거제도맛집 거제맛집 포천여행 짖어야개다 평일 포천카페 햄스터 포천이동갈비 삼척맛집 성남애견미용 근무시간 스킨케어 포천맛집 거제대명리조트맛집 성남애견호텔 골든햄스터 살롱순라 햄스타그램 2018년 삼척여행\n"
     ]
    }
   ],
   "source": [
    "pf.display_topics(lda_model14, cnt_feature_names14, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "직장인 서울맛집 강남맛집 강남 곱창 강남역 신용카드현금화 강남역맛집 카드깡 역삼맛집 역삼동맛집 아지트샵 신논현맛집 대치동맛집 곱창맛집 상품권현금화 역삼동 서초맛집 양재맛집 서초동맛집\n",
      "Topic 1:\n",
      "먹스타그램 먹방 맛스타그램 맛집 맥주스타그램 food 술스타그램 점심 먹스타 맥주 instafood foodstagram 먹방스타그램 음식 디저트 존맛 맛스타 먹부림 푸드스타그램 맛있다\n",
      "Topic 2:\n",
      "케이크토퍼 토퍼 짖어야개다 토퍼제작 생일토퍼 성남애견미용 여행토퍼 케이크번팅 성남애견호텔 생신토퍼 생신선물 hbd 성남애견유치원 백일토퍼 기념일토퍼 beautystagram banilaco 여행선물 결혼기념일토퍼 울산토퍼\n",
      "Topic 3:\n",
      "서울맛집 핫플레이스 부산맛집 맛집투어 대구맛집 JMT 서울여행 대전맛집 용산맛집 숙대맛집 남영동맛집 seoullife 맛집골목 후암동맛집 seoultravel youngsan 삼각지맛집 seouleats 서면맛집 신촌맛집\n",
      "Topic 4:\n",
      "asmr 아침식사 라이프스타일 mukbang 꾸욱 eatingshow 유튜브 첫줄안녕 tumblr 장미 eatingsounds 비지니스 스텔라코치 asmrfood asmreating 영양 먹기 Korean satisfying 열정\n",
      "Topic 5:\n",
      "우기 chomiyeon 소통해 InstaSize G_I_DLE GIDLE TFLers minnie yuqi likes4likes kpoplfl 여자아이들 TagsForLikes 영덕 miyeon cube CUBE 소연 MINNIE likesforlikesalways\n",
      "Topic 6:\n",
      "MustafaCeceli nefesimceceli antalyamcfc 햄스터 MCVeMelekleri MCveMelekleri 햄스타그램 골든햄스터 hamster music 시리안햄스터 햄스타 hamstagram hamsta syrianhamster goldenhamster 부천호빠 musician guitar 시흥호빠\n",
      "Topic 7:\n",
      "일상 맞팔 소통 데일리 선팔 셀스타그램 좋아요 셀카 셀피 팔로우 daily 먹스타그램 얼스타그램 일상스타그램 럽스타그램 선팔하면맞팔 오오티디 인친 좋아요반사 ootd\n",
      "Topic 8:\n",
      "선물 아이폰케이스 취미 마카롱 폰케이스 용인 생일 아이폰8 커플케이스 서울맛집 기념일 아이폰7케이스 아이폰x케이스 핸드메이드 귀걸이 생일선물 아이폰x 아이폰8케이스 꽃다발 악세사리\n",
      "Topic 9:\n",
      "다이어트 운동하는여자 운동 다이어터 다이어트식단 운동하는남자 헬스 diet 성남 Repost 헬스타그램 다이어트그램 식단 fitness 건강 필라테스 몸스타그램 이벤트 운스타그램 운동스타그램\n",
      "Topic 10:\n",
      "토토사이트추천 겐조 샤넬 럽스타 육아템 커플 술스타그램 구찌 에르메스 토토사이트 발렌티노 운동하는남자 팔찌 스포츠가족방 다리다리가족방 사설사이트 사다리가족방 파워볼가족방 프로토가족방 픽스터가족방\n",
      "Topic 11:\n",
      "글귀 공감 글스타그램 생각 사랑 독서 직장 감성 책스타그램 작가 음악 북스타그램 시스타그램 끄적끄적 예술 스트레스 수필 감정 자작시 직장인\n",
      "Topic 12:\n",
      "염색 헤어스타일 미용실 헤어디자이너 hair 컬러 두피케어 여자머리 두피 초대 현대아울렛가산점 독산동미용실 구로미용실 시흥사거리미용실 광명미용실 w몰 두피마사지 가산동미용실 탈모관리 곱슬교정\n",
      "Topic 13:\n",
      "축구 커플신발 아웃도어 해외직구 푸마 레플 커플운동화 쇼핑 나이키에어맥스 신발쇼핑몰 헬스 아식스 NEWBALANCE 연예인골든구스 아디다스울트라부스트 유행신발 신상신발 신발도매 GGDB 운동화도매\n",
      "Topic 14:\n",
      "럽스타그램 남친이랑 여행에미치다 여행 여행스타그램 커플 데이트 travel 제주도 korea 반려견 휴가 커플스타그램 멍스타그램 힐링 사랑해 럽스타 제주맛집 제주도맛집 강아지\n"
     ]
    }
   ],
   "source": [
    "pf.display_topics(lda_model15, cnt_feature_names15, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic별 Top 20 키워드\n",
    "\n",
    "### Topic 0: 여행스타그램\n",
    "여행에미치다 여행스타그램 여행 travel 휴가 korea photography 일본 photo seoul 바다 감성사진 한국 trip 풍경 웨딩 럽스타그램 스냅 예신 결혼\n",
    "\n",
    "### Topic 1: 대출 광고\n",
    "신용카드현금화 카드깡 상품권현금화 소액결제 소액결제현금화 모바일문화상품권 굿핀 해피머니 컬쳐랜드 신용카드대출 일수대출 모바 홍대맛집 휴대폰소액결제 차스타그램 핸드폰소액결제 빈티지 골프 상품권 비트코인\n",
    "\n",
    "### Topic 2: 운동스타그램\n",
    "다이어트 운동하는여자 다이어터 다이어트식단 용인 diet 다이어트그램 꽃다발 꽃스타그램 식단 헬스 flower 운동 운스타그램 fitness 플로리스트 유지어터 workout 운동하는남자 식단일기\n",
    "\n",
    "### Topic 3: 신발 광고\n",
    "축구 헬스 커플신발 푸마 레플 해외직구 아웃도어 커플운동화 나이키에어맥스 신발쇼핑몰 신상신발 등산화 신발도매 명품등산화 유행신발 아디다스신발 GGDB 아식스 아디다스울트라부스트 NEWBALANCE\n",
    "\n",
    "### Topic 4: Kpop\n",
    "kpop bts exo 귀여운 blackpink 여자 korea kawaii 아름다운 自撮り boy asmr korean 속초맛집 Asia jisoo kpopl4l tomboy селфи 自分撮り\n",
    "\n",
    "### Topic 5: 서울 맛집\n",
    "서울맛집 부산맛집 강남맛집 대구맛집 곱창 홍대맛집 강남역 강남역맛집 대전맛집 강남 맛스타그램 역삼맛집 이태원맛집 역삼동맛집 신논현맛집 대치동맛집 서면맛집 곱창맛집 역삼동 서초맛집\n",
    "\n",
    "### Topic 6: 럽스타그램\n",
    "남친이랑 데이트 커플 사랑해 럽스타 커플스타그램 남자친구 영화 연애중 연애 행복 남친 고마워 사랑꾼 ㅋㅋㅋ 행복해 데이트그램 사랑 여자친구 화이팅\n",
    "\n",
    "### Topic 7: 육아스타그램\n",
    "럽스타그램 육아 육아스타그램 육아소통 육아맘 맘스타그램 도치맘 일상 젊줌마 딸스타그램 인스타베이비 애스타그램 직장인 아들스타그램 사랑해 아들맘 세젤귀 반려견 워킹맘 멍스타그램\n",
    "\n",
    "### Topic 8: 토토 등 사이트 및 명품 광고\n",
    "토토사이트추천 겐조 샤넬 서울맛집 운동하는남자 럽스타 핫플레이스 커플 구찌 에르메스 술스타그램 토토사이트 발렌티노 팔찌 스포츠가족방 사설사이트추천 파워볼가족방 프로토가족방 픽스터가족방 사설사이트\n",
    "\n",
    "### Topic 9: 폰케이스 광고\n",
    "선물 직장인 아이폰케이스 냥스타그램 마카롱 취미 고양이 폰케이스 아이폰8 커플케이스 공감 그림 글스타그램 기념일 디저트 일러스트 아이폰7케이스 아이폰x케이스 글귀 귀걸이\n",
    "\n",
    "### Topic 10: 먹스타그램\n",
    "먹스타그램 먹방 맛스타그램 맛집 맥주스타그램 food 술스타그램 점심 먹스타 맥주 instafood foodstagram 먹방스타그램 음식 존맛 맛스타 먹부림 푸드스타그램 맛있다 디저트\n",
    "\n",
    "### Topic 11: 일상적인 해시태그\n",
    "일상 맞팔 데일리 소통 선팔 셀스타그램 좋아요 셀카 셀피 팔로우 먹스타그램 daily 얼스타그램 일상스타그램 선팔하면맞팔 오오티디 럽스타그램 인친 좋아요반사 ootd\n",
    "\n",
    "### Topic 12: 제주도 맛집 광고\n",
    "제주도 제주도맛집 제주맛집 제주 서귀포맛집 제주여행 f4follow 선팔맞팔 제주맛집추천 소통해요 소통하자 제주도그램 제주서귀포맛집 맥주스타그램 제주도여행 제주도흑돼지맛집 제주흑돼지 제주도흑돼지 서귀포흑돼지맛집 서귀포흑돼지\n",
    "\n",
    "### Topic 13: 거제도 맛집?\n",
    "거제도맛집 거제맛집 포천여행 짖어야개다 평일 포천카페 햄스터 포천이동갈비 삼척맛집 성남애견미용 근무시간 스킨케어 포천맛집 거제대명리조트맛집 성남애견호텔 골든햄스터 살롱순라 햄스타그램 2018년 삼척여행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1, 3, 8, 9, 12는 다 광고성 글이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_using_lda(lda_model, x_train):\n",
    "    doc_topic_dist_unnormalized = np.matrix(lda_model.transform(x_train))\n",
    "    # normalize the distribution (only needed if you want to work with the probabilities)\n",
    "    doc_topic_dist = doc_topic_dist_unnormalized/doc_topic_dist_unnormalized.sum(axis=1)\n",
    "    return doc_topic_dist.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = get_x_train_and_features_name(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = get_topic_using_lda(lda_model14, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train[train[\"tags_str\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"topic_type\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  7,  9,  0,  3,  6, 10,  5,  2,  8,  1, 13, 12,  4])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.topic_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.saveDf(df_train, \"df_train_14_topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle(\"asset/df_train_14_topic.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 577\n",
      "3 887\n",
      "8 1453\n",
      "9 1449\n",
      "12 531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4897"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_li = [1, 3, 8, 9, 12]\n",
    "count = 0\n",
    "for topic in tmp_li:\n",
    "    length = len(df_train[df_train.topic_type == topic])\n",
    "    count += length\n",
    "    print(topic, length)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"caption_only\"] = df_train[\"caption_only\"].apply(lambda a: \"\" if type(a) == float else a)\n",
    "df_train[\"caption_only\"] = df_train[\"caption_only\"].apply(lambda a: a.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = df_train[df_train[\"caption_only\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8 , 0.85, 0.9 , 0.95])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_max_df_params = np.arange(0.8, 1, 0.05)\n",
    "# tfidf_min_df_params = np.arange(0, 0.3, 0.05)\n",
    "tfidf_max_df_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32796, 3645, 32796, 3645)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_x.caption_only, train_x.topic_type, test_size=0.1, random_state=1)\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.604261796042618, total=   1.3s\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.5921413341455986, total=   1.3s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.6093845216331505, total=   1.3s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.5973788479122218, total=   1.2s\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.6022554099359951, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.5948170731707317, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.6053674900884416, total=   1.3s\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.5936546674801708, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.5963358778625955, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.6036063569682152, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.6146118721461187, total=   3.0s\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.605239110569601, total=   3.0s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.621267519804997, total=   3.0s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.6080463273392258, total=   3.1s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.6083511124657117, total=   3.1s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.6048780487804878, total=   3.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.6181762732540409, total=   3.0s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.5933496034167175, total=   3.0s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.6070229007633587, total=   3.1s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.6204156479217604, total=   3.1s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6155251141552511, total=   4.8s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6082851050868109, total=   4.9s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6230956733698964, total=   5.0s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6144468149954282, total=   5.0s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6113989637305699, total=   4.9s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6012195121951219, total=   5.0s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:   37.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6187862153095456, total=   5.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.5979255643685174, total=   5.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.604261796042618, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6058015267175573, total=   5.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.5921413341455986, total=   1.3s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.6093845216331505, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6219437652811736, total=   5.1s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.5973788479122218, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.6022554099359951, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.5948170731707317, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.6053674900884416, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.5936546674801708, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.5963358778625955, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.6036063569682152, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.6146118721461187, total=   3.1s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.605239110569601, total=   3.1s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.621267519804997, total=   3.3s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.6080463273392258, total=   3.3s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.6083511124657117, total=   3.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.6048780487804878, total=   3.1s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.6181762732540409, total=   3.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.5933496034167175, total=   3.1s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.6070229007633587, total=   3.1s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.6204156479217604, total=   3.1s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6155251141552511, total=   5.3s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6082851050868109, total=   5.3s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6230956733698964, total=   4.9s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6144468149954282, total=   4.9s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6113989637305699, total=   4.9s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6012195121951219, total=   4.9s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6187862153095456, total=   5.0s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.5979255643685174, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.604261796042618, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6058015267175573, total=   4.8s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.5921413341455986, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.6093845216331505, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.5973788479122218, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6219437652811736, total=   4.8s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.6022554099359951, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.5948170731707317, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.6053674900884416, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.5936546674801708, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.5963358778625955, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.6036063569682152, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.6146118721461187, total=   3.1s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.605239110569601, total=   3.1s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.621267519804997, total=   3.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.6080463273392258, total=   3.0s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.6083511124657117, total=   3.0s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.6048780487804878, total=   3.0s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.6181762732540409, total=   3.0s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.5933496034167175, total=   3.0s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.6070229007633587, total=   3.0s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.6204156479217604, total=   3.0s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6155251141552511, total=   4.8s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6082851050868109, total=   4.8s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6230956733698964, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6144468149954282, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6113989637305699, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6012195121951219, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6187862153095456, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.5979255643685174, total=   4.8s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.604261796042618, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.5921413341455986, total=   1.4s\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6058015267175573, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.6093845216331505, total=   1.3s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.5973788479122218, total=   1.3s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6219437652811736, total=   5.3s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.6022554099359951, total=   1.3s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.5948170731707317, total=   1.3s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.6053674900884416, total=   1.3s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.5936546674801708, total=   1.4s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.5963358778625955, total=   1.4s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.6036063569682152, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.6146118721461187, total=   3.4s\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.605239110569601, total=   3.4s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.621267519804997, total=   3.4s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.6080463273392258, total=   3.1s\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.6083511124657117, total=   3.1s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.6048780487804878, total=   3.0s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.6181762732540409, total=   3.1s\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.5933496034167175, total=   3.1s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.6070229007633587, total=   3.0s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.6204156479217604, total=   3.1s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6155251141552511, total=   5.0s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6082851050868109, total=   5.3s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6230956733698964, total=   5.8s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6144468149954282, total=   5.6s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6113989637305699, total=   5.6s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6012195121951219, total=   5.5s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6187862153095456, total=   5.4s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.5979255643685174, total=   5.4s\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6058015267175573, total=   5.5s\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6219437652811736, total=   5.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 120 out of 120 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...ue,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=3,\n",
       "       param_grid={'tfidf__max_df': array([0.8 , 0.85, 0.9 , 0.95]), 'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB(alpha=0.01)) \n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': tfidf_max_df_params,\n",
    "#     'tfidf__min_df': tfidf_min_df_params,\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]\n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=10, n_jobs=3, verbose=3)\n",
    "\n",
    "# %%time\n",
    "grid_search_tune.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best parameters set:\")\n",
    "# print(grid_search_tune.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'tfidf__max_df': 0.8, 'tfidf__ngram_range': (1, 3)}, 0.6118429076716673)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tune.best_params_, grid_search_tune.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6118429076716673"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tune.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([ \n",
    "    ('vect', TfidfVectorizer(ngram_range=(1, 3))), \n",
    "    ('clf', MultinomialNB(alpha=0.01)) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.73 s, sys: 303 ms, total: 5.03 s\n",
      "Wall time: 5.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_all_data = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_all_data.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.36      0.44       198\n",
      "          1       0.96      0.72      0.82        67\n",
      "          2       0.67      0.22      0.33        55\n",
      "          3       0.40      0.08      0.13        75\n",
      "          4       0.85      0.39      0.54        28\n",
      "          5       0.78      0.69      0.73       143\n",
      "          6       0.33      0.12      0.18       174\n",
      "          7       0.53      0.45      0.49       343\n",
      "          8       0.92      0.94      0.93       131\n",
      "          9       0.76      0.42      0.55       153\n",
      "         10       0.50      0.58      0.54       573\n",
      "         11       0.65      0.80      0.72      1632\n",
      "         12       0.92      0.59      0.72        58\n",
      "         13       0.89      0.53      0.67        15\n",
      "\n",
      "avg / total       0.62      0.63      0.61      3645\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle imbalance problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_imbalance(method, x_train, y_train, x_test, y_test, rs=0, use_exact_model = []):\n",
    "    method_idx = 0\n",
    "    if method == \"under\":\n",
    "        method_idx = 0\n",
    "    elif method == \"over\":\n",
    "        method_idx = 1\n",
    "    elif method == \"combine\":\n",
    "        method_idx = 2\n",
    "    else:\n",
    "        print(\"Error occured.\")\n",
    "        return\n",
    "    multimodel = MultinomialNB(alpha=0.01)\n",
    "    tf_vec = TfidfVectorizer(ngram_range=(1, 3))\n",
    "    x_sample = tf_vec.fit_transform(x_train)\n",
    "    models = [\n",
    "        [\n",
    "            RandomUnderSampler(random_state=rs),\n",
    "            TomekLinks(random_state=rs),\n",
    "            CondensedNearestNeighbour(random_state=rs),\n",
    "            OneSidedSelection(random_state=rs),\n",
    "            EditedNearestNeighbours(random_state=rs),\n",
    "            NeighbourhoodCleaningRule(random_state=rs)\n",
    "        ],\n",
    "        [\n",
    "            RandomOverSampler(random_state=rs),\n",
    "            ADASYN(random_state=rs),\n",
    "            SMOTE(random_state=rs)\n",
    "        ],\n",
    "        [\n",
    "            SMOTEENN(random_state=rs),\n",
    "            SMOTETomek(random_state=rs)\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    if len(use_exact_model) != 0:\n",
    "        method_idx = use_exact_model[0]\n",
    "        model_idx = use_exact_model[1]\n",
    "        x_random, y_random = models[method_idx][model_idx].fit_sample(x_sample, y_train)\n",
    "        multimodel.fit(x_random, y_random)\n",
    "        X_test = tf_vec.transform(x_test)\n",
    "        y_pred = multimodel.predict(X_test)\n",
    "        print(str(idx) + \" 번째 모델 Accuracy 결과 : \")\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "        return multimodel\n",
    "        \n",
    "    print(method + \" sampling started.\")\n",
    "    print(\"Total sampler length : \" + str(len(models[method_idx])))\n",
    "    for idx, model in enumerate(models[method_idx]):\n",
    "        x_random, y_random = model.fit_sample(x_sample, y_train)\n",
    "        multimodel.fit(x_random, y_random)\n",
    "        X_test = tf_vec.transform(x_test)\n",
    "        y_pred = multimodel.predict(X_test)\n",
    "        print(str(idx) + \" 번째 모델 Accuracy 결과 : \")\n",
    "        print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.62716으로 TomekLinks를 사용하면 성능이 조금 더 좋아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "under sampling started.\n",
      "0 번째 모델 Accuracy 결과 : \n",
      "0.3574759945130315\n",
      "1 번째 모델 Accuracy 결과 : \n",
      "0.6271604938271605\n",
      "2 번째 모델 Accuracy 결과 : \n",
      "0.17366255144032922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 번째 모델 Accuracy 결과 : \n",
      "0.6183813443072702\n",
      "4 번째 모델 Accuracy 결과 : \n",
      "0.5432098765432098\n",
      "5 번째 모델 Accuracy 결과 : \n",
      "0.574485596707819\n"
     ]
    }
   ],
   "source": [
    "handle_imbalance(\"under\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over sampler 로는 개선된 점이 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over sampling started.\n",
      "0 번째 모델 Accuracy 결과 : \n",
      "0.5097393689986283\n",
      "1 번째 모델 Accuracy 결과 : \n",
      "0.5026063100137175\n",
      "2 번째 모델 Accuracy 결과 : \n",
      "0.5111111111111111\n"
     ]
    }
   ],
   "source": [
    "handle_imbalance(\"over\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Sampling을 통해서 오히려 성적이 떨어졌고, Combine Sampler들은 현재 용량부족으로 결과를 내지 못하여 일단 보류했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combine sampling started.\n",
      "Total sampler length : 2\n"
     ]
    }
   ],
   "source": [
    "handle_imbalance(\"combine\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = handle_imbalance(\"under\", X_train, y_train, X_test, y_test, [0, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
