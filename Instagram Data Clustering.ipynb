{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project : Instagram Data Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Project Introduce\n",
    "### 1.1 Purpose : \n",
    "문장이 주어지면 주제(Topic)을 알아내는 프로젝트입니다.\n",
    "\n",
    "그로인한 기대효과로 크게는\n",
    "1. 사용자가 쓰는 글 내용의 Topic을 알아내고 분석하여 추천 시스템의 기반을 마련하는 것\n",
    "2. 결정된 Topic이 가진 단어 분포(우리는 Hashtag를 통해서 모델링 했으므로 이 경우는 Hashtag를 의미함)를 이용해서 Hashtag를 생성\n",
    "\n",
    "이렇게 2 가지가 있습니다.\n",
    "\n",
    "# 2. How\n",
    "1. feature vector로는 유저가 각 포스팅에 사용한 Hashtag를 Countvectorizer를 통해서 인코딩한 것을 사용했습니다.\n",
    "2. Latent Dirichlet Allocation (LDA) algorithm을 사용해서 클러스터링을 진행했고, 그 가장 높은 Topic 결과를 각 문장의 Topic column(Target column)으로 설정하였습니다.\n",
    "3. 사용자가 쓴 글 내용(Caption)에서 tag는 제외하고(caption_only)을 다시 tf-idf vectorizer로 인코딩하여 featrue vector로 삼고 Machine learning 기법들(Multinomial Naive Bayse, XGBoost 등)을 활용해서 학습시켰습니다.\n",
    "  \n",
    "# 2. Data Source\n",
    "### 2.1 Scrapy :\n",
    "Scrapy framework를 사용하여 Instagram Hastag로 검색된 결과들을 Crawling 할 수 있는 Crawler를 설계하였습니다.\n",
    "<img src='./scrapy_screenshot.png'>\n",
    "\n",
    "> Instagram은 자신들에의해 검증된 어플리케이션 외에는 API를 제공하지 않습니다. 그래서 ```https://www.instagram.com/explore/tags/{tag_name}/?__a=1``` 의 방법을 사용하여 크롤링을 진행하였습니다.\n",
    "    \n",
    "# 3. Data Introduce\n",
    "### 3.1 Data set :\n",
    "   > - Train Data : 38791 rows, 20 columns\n",
    "   \n",
    "### 3.2 Columns : \n",
    "- 포스팅 json에서 추출해낸 columns\n",
    "    - caption : 원본 글 내용\n",
    "    - comment_cnt : 댓글 개수\n",
    "    - first_comment : 첫 댓글(본인이 남긴 경우는 보통 Tag인 경우가 대부분)\n",
    "    - id : 글 ID\n",
    "    - is_video : 동영상인지 여부\n",
    "    - likes : 좋아요 개수\n",
    "    - loc_id : location id\n",
    "    - loc_lat : 위도\n",
    "    - loc_lon : 경도\n",
    "    - owner_id : 유저 ID\n",
    "    - owner_name : 닉네임\n",
    "    - shortcode : 글 고유 번호\n",
    "    - taken_at_timestamp : 저장된 시간\n",
    "    - video_view_count : 동영상 조회 횟수\n",
    "    \n",
    "- caption을 이용해서 만들어낸 columns\n",
    "    - tags : 사용한 hashtag list\n",
    "    - tags_cnt : hashtag 개수\n",
    "    - caption_only : 태그 제외한 글 내용\n",
    "    - tags_str : hashtag 문자열\n",
    "    - duplicated_tag : 중복된 태그가 있는지 여부(보통은 두번 쓰지않음)\n",
    "    \n",
    "# 4. Algorithms\n",
    "### 4.1 LDA Topic Modeling\n",
    "sklearn의 decomposition.LatentDirichletAllocation class를 사용\n",
    "\n",
    "### 4.2 Multinomial Naive Bayes Classifier\n",
    "sklearn.naive_bayes.MultinomialNB class를 사용\n",
    "\n",
    "### 4.3 Hyperparameter Tunning\n",
    "sklearn.model_selection.GridSearchCV class를 사용\n",
    "\n",
    "# 5. Conculsion(2019. 1. 21일 기준)\n",
    "### 5.1 Problems\n",
    "1. Hashtag의 남용. 사용자들이 사진과 관련없는 내용도 tag에 추가하는 경우가 많았습니다. (#일상#ootd#f4f#선팔맞팔#.. 등등)\n",
    "    - 해결법 : \n",
    "        1. 이 문제의 해결법으로 많이 사용되는 tag들은 제거하고 Topic 모델링을 해서 좀 더 핵심이 되는 tag들에 의존하는 방법을 사용할 수 있습니다.\n",
    "        2. LDA 모델링 후, 머신러닝을 통해서 caption을 형태소 분석기를 통해서 좀 더 정밀한 전처리를 실시하는 방법이 있습니다.\n",
    "        \n",
    "2. 비대칭 데이터.\n",
    "    - 해결법 : Oversampling이나 Undersampling을 이용합니다.\n",
    "\n",
    "### 5.2 Result\n",
    "결과는 현재 이상적이지 않습니다. Accuracy 0.62716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from IPython.display import display, Markdown\n",
    "import private_function as pf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.under_sampling import *\n",
    "from imblearn.over_sampling import *\n",
    "from imblearn.combine import *\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_x_train_and_features_name(df):\n",
    "    cnt_vectorizer = CountVectorizer(lowercase=False)\n",
    "    X_train = cnt_vectorizer.fit_transform(df)\n",
    "#     cnt_feature_names = cnt_vectorizer.get_feature_names()\n",
    "    return X_train\n",
    "\n",
    "def run_lda(df, n_topic, max_iter = 100):\n",
    "    cnt_vectorizer = CountVectorizer(lowercase=False)\n",
    "    X_train = cnt_vectorizer.fit_transform(df)\n",
    "    cnt_feature_names = cnt_vectorizer.get_feature_names()\n",
    "\n",
    "    # hyper parameter\n",
    "    alpha = 1\n",
    "    beta = 1\n",
    "\n",
    "    # train the model\n",
    "    lda = LatentDirichletAllocation(n_components=n_topic, doc_topic_prior=alpha,\\\n",
    "                                    topic_word_prior=beta, learning_method='online', max_iter=max_iter)\n",
    "\n",
    "    %time lda.fit_transform(X_train)\n",
    "    \n",
    "    return lda, cnt_vectorizer, cnt_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "['직장인', '럽스타그램', '서울맛집', '먹스타그램', '친스타그램', '여행', '일상', '셀스타그램']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "['seoul']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "job.csv : 2213"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_new_5.csv : 2136"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_new_4.csv : 2148"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_new_3.csv : 2172"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_new_2.csv : 465"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_new_1.csv : 2029"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_new.csv : 2203"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_train.csv : 5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_food_1.csv : 2217"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "insta_food.csv : 1911"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "food.csv : 1986"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "beer.csv : 1970"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "food_1.csv : 2068"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "food_2.csv : 2089"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "food_3.csv : 2046"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "friend.csv : 1888"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "trip_1.csv : 1145"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "trip.csv : 2533"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "daily.csv : 2195"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "daily_2.csv : 785"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "daily_1.csv : 2312"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "selfie.csv : 2156"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "seongnam.csv : 960"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "seoul_2.csv : 19"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "seoul_1.csv : 960"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "incheon.csv : 960"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "seoul.csv : 880"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "yongin.csv : 960"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "gyeonggido.csv : 960"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "Df list length : 29\n",
      "Df length changes after concat (if 0 means all datas are unique) : 0\n",
      "Df length changes after concat (if 0 means all datas are unique) : 0\n",
      "Df length changes after concat (if 0 means all datas are unique) : 1095\n",
      "Df length changes after concat (if 0 means all datas are unique) : 411\n",
      "Df length changes after concat (if 0 means all datas are unique) : 54\n",
      "Df length changes after concat (if 0 means all datas are unique) : 131\n",
      "Df length changes after concat (if 0 means all datas are unique) : 0\n",
      "Df length changes after concat (if 0 means all datas are unique) : 35\n",
      "Df length changes after concat (if 0 means all datas are unique) : 16\n",
      "Df length changes after concat (if 0 means all datas are unique) : 75\n",
      "Df length changes after concat (if 0 means all datas are unique) : 11\n",
      "Df length changes after concat (if 0 means all datas are unique) : 42\n",
      "Df length changes after concat (if 0 means all datas are unique) : 358\n",
      "Df length changes after concat (if 0 means all datas are unique) : 19\n",
      "Df length changes after concat (if 0 means all datas are unique) : 176\n",
      "Df length changes after concat (if 0 means all datas are unique) : 15\n",
      "Df length changes after concat (if 0 means all datas are unique) : 0\n",
      "Df length changes after concat (if 0 means all datas are unique) : 55\n",
      "Df length changes after concat (if 0 means all datas are unique) : 20\n",
      "Df length changes after concat (if 0 means all datas are unique) : 304\n",
      "Df length changes after concat (if 0 means all datas are unique) : 286\n",
      "Df length changes after concat (if 0 means all datas are unique) : 0\n",
      "Df length changes after concat (if 0 means all datas are unique) : 0\n",
      "Df length changes after concat (if 0 means all datas are unique) : 1\n",
      "Df length changes after concat (if 0 means all datas are unique) : 1\n",
      "Df length changes after concat (if 0 means all datas are unique) : 1\n",
      "Df length changes after concat (if 0 means all datas are unique) : 0\n",
      "Df length changes after concat (if 0 means all datas are unique) : 1\n",
      "df_train shape : (43264, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(46899, 14241)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_li = pf.getAllDataFrame()\n",
    "train = pf.makeOneTrainDf(df_li)\n",
    "train = pf.make_df_i_want(train)\n",
    "counter = pf.get_counter(train)\n",
    "counter.most_common()\n",
    "tag_count = 0\n",
    "for tup in counter.most_common():\n",
    "    tag_count += tup[1]\n",
    "tag_count, len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train[train.tags_cnt != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38791, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lda_model(n_topic):\n",
    "    n_topic = str(n_topic)\n",
    "    lda = joblib.load(\"./lda_model\" + n_topic + \".pkl\")\n",
    "    cnt_feature_name = joblib.load(\"./lda_model\" + n_topic + \"_feature_name.pkl\")\n",
    "    return lda, cnt_feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 406 ms, sys: 29.2 ms, total: 436 ms\n",
      "Wall time: 442 ms\n"
     ]
    }
   ],
   "source": [
    "%time lda_model14, cnt_feature_names14 = load_lda_model(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Modeling 후 결론\n",
    "\n",
    "> Topic 개수는 11-15개까지 바꿔가면서 시도했습니다. 문제는 최적의 Topic 개수를 찾는 것이었는데, Perplexity가 Topic을 늘릴수록 늘어났습니다. 그럼에도 14개를 선택한 것은 직접 눈으로 보고 판단하여 가장 합리적으로 잘 분류됐다고 판단했기 때문입니다. Topic별 Top 20개의 Hashtag를 살펴보면 아래와 같습니다.\n",
    "\n",
    "### Topic 0: 여행스타그램\n",
    "여행에미치다 여행스타그램 여행 travel 휴가 korea photography 일본 photo seoul 바다 감성사진 한국 trip 풍경 웨딩 럽스타그램 스냅 예신 결혼\n",
    "\n",
    "### Topic 1: 대출 광고\n",
    "신용카드현금화 카드깡 상품권현금화 소액결제 소액결제현금화 모바일문화상품권 굿핀 해피머니 컬쳐랜드 신용카드대출 일수대출 모바 홍대맛집 휴대폰소액결제 차스타그램 핸드폰소액결제 빈티지 골프 상품권 비트코인\n",
    "\n",
    "### Topic 2: 운동스타그램\n",
    "다이어트 운동하는여자 다이어터 다이어트식단 용인 diet 다이어트그램 꽃다발 꽃스타그램 식단 헬스 flower 운동 운스타그램 fitness 플로리스트 유지어터 workout 운동하는남자 식단일기\n",
    "\n",
    "### Topic 3: 신발 광고\n",
    "축구 헬스 커플신발 푸마 레플 해외직구 아웃도어 커플운동화 나이키에어맥스 신발쇼핑몰 신상신발 등산화 신발도매 명품등산화 유행신발 아디다스신발 GGDB 아식스 아디다스울트라부스트 NEWBALANCE\n",
    "\n",
    "### Topic 4: Kpop\n",
    "kpop bts exo 귀여운 blackpink 여자 korea kawaii 아름다운 自撮り boy asmr korean 속초맛집 Asia jisoo kpopl4l tomboy селфи 自分撮り\n",
    "\n",
    "### Topic 5: 서울 맛집\n",
    "서울맛집 부산맛집 강남맛집 대구맛집 곱창 홍대맛집 강남역 강남역맛집 대전맛집 강남 맛스타그램 역삼맛집 이태원맛집 역삼동맛집 신논현맛집 대치동맛집 서면맛집 곱창맛집 역삼동 서초맛집\n",
    "\n",
    "### Topic 6: 럽스타그램\n",
    "남친이랑 데이트 커플 사랑해 럽스타 커플스타그램 남자친구 영화 연애중 연애 행복 남친 고마워 사랑꾼 ㅋㅋㅋ 행복해 데이트그램 사랑 여자친구 화이팅\n",
    "\n",
    "### Topic 7: 육아스타그램\n",
    "럽스타그램 육아 육아스타그램 육아소통 육아맘 맘스타그램 도치맘 일상 젊줌마 딸스타그램 인스타베이비 애스타그램 직장인 아들스타그램 사랑해 아들맘 세젤귀 반려견 워킹맘 멍스타그램\n",
    "\n",
    "### Topic 8: 토토 등 사이트 및 명품 광고\n",
    "토토사이트추천 겐조 샤넬 서울맛집 운동하는남자 럽스타 핫플레이스 커플 구찌 에르메스 술스타그램 토토사이트 발렌티노 팔찌 스포츠가족방 사설사이트추천 파워볼가족방 프로토가족방 픽스터가족방 사설사이트\n",
    "\n",
    "### Topic 9: 폰케이스 광고\n",
    "선물 직장인 아이폰케이스 냥스타그램 마카롱 취미 고양이 폰케이스 아이폰8 커플케이스 공감 그림 글스타그램 기념일 디저트 일러스트 아이폰7케이스 아이폰x케이스 글귀 귀걸이\n",
    "\n",
    "### Topic 10: 먹스타그램\n",
    "먹스타그램 먹방 맛스타그램 맛집 맥주스타그램 food 술스타그램 점심 먹스타 맥주 instafood foodstagram 먹방스타그램 음식 존맛 맛스타 먹부림 푸드스타그램 맛있다 디저트\n",
    "\n",
    "### Topic 11: 일상적인 해시태그\n",
    "일상 맞팔 데일리 소통 선팔 셀스타그램 좋아요 셀카 셀피 팔로우 먹스타그램 daily 얼스타그램 일상스타그램 선팔하면맞팔 오오티디 럽스타그램 인친 좋아요반사 ootd\n",
    "\n",
    "### Topic 12: 제주도 맛집 광고\n",
    "제주도 제주도맛집 제주맛집 제주 서귀포맛집 제주여행 f4follow 선팔맞팔 제주맛집추천 소통해요 소통하자 제주도그램 제주서귀포맛집 맥주스타그램 제주도여행 제주도흑돼지맛집 제주흑돼지 제주도흑돼지 서귀포흑돼지맛집 서귀포흑돼지\n",
    "\n",
    "### Topic 13: 거제도 맛집?\n",
    "거제도맛집 거제맛집 포천여행 짖어야개다 평일 포천카페 햄스터 포천이동갈비 삼척맛집 성남애견미용 근무시간 스킨케어 포천맛집 거제대명리조트맛집 성남애견호텔 골든햄스터 살롱순라 햄스타그램 2018년 삼척여행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1, 3, 8, 9, 12는 다 광고성 글이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_using_lda(lda_model, x_train):\n",
    "    doc_topic_dist_unnormalized = np.matrix(lda_model.transform(x_train))\n",
    "    # normalize the distribution (only needed if you want to work with the probabilities)\n",
    "    doc_topic_dist = doc_topic_dist_unnormalized/doc_topic_dist_unnormalized.sum(axis=1)\n",
    "    return doc_topic_dist.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[df_train[\"tags_str\"] != \"\"][\"tags_str\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = get_x_train_and_features_name(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = get_topic_using_lda(lda_model14, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train[train[\"tags_str\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"topic_type\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  7,  9,  0,  3,  6, 10,  5,  2,  8,  1, 13, 12,  4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.topic_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.saveDf(df_train, \"df_train_14_topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle(\"asset/df_train_14_topic.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 577\n",
      "3 887\n",
      "8 1453\n",
      "9 1449\n",
      "12 531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4897"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_li = [1, 3, 8, 9, 12]\n",
    "count = 0\n",
    "for topic in tmp_li:\n",
    "    length = len(df_train[df_train.topic_type == topic])\n",
    "    count += length\n",
    "    print(topic, length)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 광고에 해당하는 것으로 보이는 데이터가 4897개이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"caption_only\"] = df_train[\"caption_only\"].apply(lambda a: \"\" if type(a) == float else a)\n",
    "df_train[\"caption_only\"] = df_train[\"caption_only\"].apply(lambda a: a.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = df_train[df_train[\"caption_only\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8 , 0.85, 0.9 , 0.95])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_max_df_params = np.arange(0.8, 1, 0.05)\n",
    "# tfidf_min_df_params = np.arange(0, 0.3, 0.05)\n",
    "tfidf_max_df_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32796, 3645, 32796, 3645)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_x.caption_only, train_x.topic_type, test_size=0.1, random_state=1)\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.604261796042618, total=   1.3s\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.5921413341455986, total=   1.3s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.6093845216331505, total=   1.3s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.5973788479122218, total=   1.3s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.6022554099359951, total=   1.4s\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.5948170731707317, total=   1.3s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.6053674900884416, total=   1.3s\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.5936546674801708, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.5963358778625955, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 1), score=0.6036063569682152, total=   1.4s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.6146118721461187, total=   3.3s\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.605239110569601, total=   3.3s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.621267519804997, total=   3.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.6080463273392258, total=   3.1s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.6083511124657117, total=   3.1s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.6048780487804878, total=   3.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.6181762732540409, total=   3.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.5933496034167175, total=   3.1s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.6070229007633587, total=   3.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 2), score=0.6204156479217604, total=   3.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6155251141552511, total=   5.3s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6082851050868109, total=   5.5s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6230956733698964, total=   5.3s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6144468149954282, total=   5.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6113989637305699, total=   5.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6012195121951219, total=   5.3s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:   39.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6187862153095456, total=   5.2s\n",
      "[CV] tfidf__max_df=0.8, tfidf__ngram_range=(1, 3) ....................\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.5979255643685174, total=   5.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.604261796042618, total=   1.4s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6058015267175573, total=   5.3s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.5921413341455986, total=   1.3s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.6093845216331505, total=   1.4s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8, tfidf__ngram_range=(1, 3), score=0.6219437652811736, total=   5.6s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.5973788479122218, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.6022554099359951, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.5948170731707317, total=   1.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.6053674900884416, total=   1.3s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.5936546674801708, total=   1.4s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.5963358778625955, total=   1.4s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 1), score=0.6036063569682152, total=   1.3s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.6146118721461187, total=   3.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.605239110569601, total=   3.1s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.621267519804997, total=   3.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.6080463273392258, total=   3.3s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.6083511124657117, total=   3.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.6048780487804878, total=   3.2s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.6181762732540409, total=   3.0s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.5933496034167175, total=   3.0s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.6070229007633587, total=   3.0s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 2), score=0.6204156479217604, total=   3.0s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6155251141552511, total=   4.8s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6082851050868109, total=   4.8s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6230956733698964, total=   4.9s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6144468149954282, total=   4.8s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6113989637305699, total=   4.8s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6012195121951219, total=   5.5s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6187862153095456, total=   5.5s\n",
      "[CV] tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.5979255643685174, total=   5.3s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.604261796042618, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6058015267175573, total=   5.0s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.5921413341455986, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.6093845216331505, total=   1.3s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.5973788479122218, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.8500000000000001, tfidf__ngram_range=(1, 3), score=0.6219437652811736, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.6022554099359951, total=   1.3s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.5948170731707317, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.6053674900884416, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.5936546674801708, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.5963358778625955, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 1), score=0.6036063569682152, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.6146118721461187, total=   3.1s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.605239110569601, total=   3.1s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.621267519804997, total=   3.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.6080463273392258, total=   3.4s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.6083511124657117, total=   3.4s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.6048780487804878, total=   3.4s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.6181762732540409, total=   3.5s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.5933496034167175, total=   3.5s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.6070229007633587, total=   3.5s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 2), score=0.6204156479217604, total=   3.5s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6155251141552511, total=   5.8s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6082851050868109, total=   5.8s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6230956733698964, total=   5.6s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6144468149954282, total=   5.3s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6113989637305699, total=   5.3s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6012195121951219, total=   4.8s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6187862153095456, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.5979255643685174, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.604261796042618, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6058015267175573, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.5921413341455986, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.6093845216331505, total=   1.2s\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.5973788479122218, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9000000000000001, tfidf__ngram_range=(1, 3), score=0.6219437652811736, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.6022554099359951, total=   1.2s\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.5948170731707317, total=   1.1s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.6053674900884416, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.5936546674801708, total=   1.2s\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.5963358778625955, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 1), score=0.6036063569682152, total=   1.2s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.6146118721461187, total=   3.1s\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.605239110569601, total=   3.1s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.621267519804997, total=   3.1s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.6080463273392258, total=   3.1s\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.6083511124657117, total=   3.1s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.6048780487804878, total=   3.0s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.6181762732540409, total=   3.1s\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.5933496034167175, total=   3.1s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2) .....\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.6070229007633587, total=   3.0s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 2), score=0.6204156479217604, total=   3.1s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6155251141552511, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6082851050868109, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6230956733698964, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6144468149954282, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6113989637305699, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6012195121951219, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6187862153095456, total=   4.9s\n",
      "[CV] tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.5979255643685174, total=   4.9s\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6058015267175573, total=   4.8s\n",
      "[CV]  tfidf__max_df=0.9500000000000002, tfidf__ngram_range=(1, 3), score=0.6219437652811736, total=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 120 out of 120 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...ue,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=3,\n",
       "       param_grid={'tfidf__max_df': array([0.8 , 0.85, 0.9 , 0.95]), 'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB(alpha=0.01)) \n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': tfidf_max_df_params,\n",
    "#     'tfidf__min_df': tfidf_min_df_params,\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]\n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=10, n_jobs=3, verbose=3)\n",
    "\n",
    "# %%time\n",
    "grid_search_tune.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best parameters set:\")\n",
    "# print(grid_search_tune.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'tfidf__max_df': 0.8, 'tfidf__ngram_range': (1, 3)}, 0.6118429076716673)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tune.best_params_, grid_search_tune.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best score를 낸 parameter max_df : 0.8, ngram_range : (1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6118429076716673"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tune.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([ \n",
    "    ('vect', TfidfVectorizer(ngram_range=(1, 3))), \n",
    "    ('clf', MultinomialNB(alpha=0.01)) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.75 s, sys: 275 ms, total: 5.02 s\n",
      "Wall time: 5.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_all_data = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_all_data.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.36      0.44       198\n",
      "          1       0.96      0.72      0.82        67\n",
      "          2       0.67      0.22      0.33        55\n",
      "          3       0.40      0.08      0.13        75\n",
      "          4       0.85      0.39      0.54        28\n",
      "          5       0.78      0.69      0.73       143\n",
      "          6       0.33      0.12      0.18       174\n",
      "          7       0.53      0.45      0.49       343\n",
      "          8       0.92      0.94      0.93       131\n",
      "          9       0.76      0.42      0.55       153\n",
      "         10       0.50      0.58      0.54       573\n",
      "         11       0.65      0.80      0.72      1632\n",
      "         12       0.92      0.59      0.72        58\n",
      "         13       0.89      0.53      0.67        15\n",
      "\n",
      "avg / total       0.62      0.63      0.61      3645\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle imbalance problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_imbalance(method, x_train, y_train, x_test, y_test, rs=0, use_exact_model = []):\n",
    "    method_idx = 0\n",
    "    if method == \"under\":\n",
    "        method_idx = 0\n",
    "    elif method == \"over\":\n",
    "        method_idx = 1\n",
    "    elif method == \"combine\":\n",
    "        method_idx = 2\n",
    "    else:\n",
    "        print(\"Error occured.\")\n",
    "        return\n",
    "    multimodel = MultinomialNB(alpha=0.01)\n",
    "    tf_vec = TfidfVectorizer(ngram_range=(1, 3))\n",
    "    x_sample = tf_vec.fit_transform(x_train)\n",
    "    models = [\n",
    "        [\n",
    "            RandomUnderSampler(random_state=rs),\n",
    "            TomekLinks(random_state=rs),\n",
    "            CondensedNearestNeighbour(random_state=rs),\n",
    "            OneSidedSelection(random_state=rs),\n",
    "            EditedNearestNeighbours(random_state=rs),\n",
    "            NeighbourhoodCleaningRule(random_state=rs)\n",
    "        ],\n",
    "        [\n",
    "            RandomOverSampler(random_state=rs),\n",
    "            ADASYN(random_state=rs),\n",
    "            SMOTE(random_state=rs)\n",
    "        ],\n",
    "        [\n",
    "            SMOTEENN(random_state=rs),\n",
    "            SMOTETomek(random_state=rs)\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    if len(use_exact_model) != 0:\n",
    "        method_idx = use_exact_model[0]\n",
    "        model_idx = use_exact_model[1]\n",
    "        x_random, y_random = models[method_idx][model_idx].fit_sample(x_sample, y_train)\n",
    "        multimodel.fit(x_random, y_random)\n",
    "        X_test = tf_vec.transform(x_test)\n",
    "        y_pred = multimodel.predict(X_test)\n",
    "        print(str(idx) + \" 번째 모델 Accuracy 결과 : \")\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "        return multimodel\n",
    "        \n",
    "    print(method + \" sampling started.\")\n",
    "    print(\"Total sampler length : \" + str(len(models[method_idx])))\n",
    "    for idx, model in enumerate(models[method_idx]):\n",
    "        x_random, y_random = model.fit_sample(x_sample, y_train)\n",
    "        multimodel.fit(x_random, y_random)\n",
    "        X_test = tf_vec.transform(x_test)\n",
    "        y_pred = multimodel.predict(X_test)\n",
    "        print(str(idx) + \" 번째 모델 Accuracy 결과 : \")\n",
    "        print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.62716으로 TomekLinks를 사용하면 성능이 조금 더 좋아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "under sampling started.\n",
      "0 번째 모델 Accuracy 결과 : \n",
      "0.3574759945130315\n",
      "1 번째 모델 Accuracy 결과 : \n",
      "0.6271604938271605\n",
      "2 번째 모델 Accuracy 결과 : \n",
      "0.17366255144032922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n",
      "/usr/local/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/one_sided_selection.py:197: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  idx_maj_extracted = np.delete(idx_maj, idx_maj_sample, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 번째 모델 Accuracy 결과 : \n",
      "0.6183813443072702\n",
      "4 번째 모델 Accuracy 결과 : \n",
      "0.5432098765432098\n",
      "5 번째 모델 Accuracy 결과 : \n",
      "0.574485596707819\n"
     ]
    }
   ],
   "source": [
    "handle_imbalance(\"under\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over sampler 로는 개선된 점이 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over sampling started.\n",
      "0 번째 모델 Accuracy 결과 : \n",
      "0.5097393689986283\n",
      "1 번째 모델 Accuracy 결과 : \n",
      "0.5026063100137175\n",
      "2 번째 모델 Accuracy 결과 : \n",
      "0.5111111111111111\n"
     ]
    }
   ],
   "source": [
    "handle_imbalance(\"over\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Sampling을 통해서 오히려 성적이 떨어졌고, Combine Sampler들은 현재 용량부족으로 결과를 내지 못하여 일단 보류했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combine sampling started.\n",
      "Total sampler length : 2\n"
     ]
    }
   ],
   "source": [
    "handle_imbalance(\"combine\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = handle_imbalance(\"under\", X_train, y_train, X_test, y_test, [0, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
